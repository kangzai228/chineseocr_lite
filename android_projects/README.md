## 以下内容仅代表benjaminwan个人观点 仅供参考

#### android平台onnxruntime vs ncnn对比测试介绍
1. ChOcrLiteAndroidOnnx项目使用onnxruntime框架，ChOcrLiteAndroidOnnxToNcnn项目使用ncnn框架，用于对比的项目JNI部分均使用C++编写，JVM部分使用Kotlin编写。
2. 除了调用推理框架的代码，其它代码尽量保持一致。
3. 推理框架均不使用GPU或NPU加速，仅使用CPU进行计算。
4. 模型使用chineseocr-lite的onnx分支的模型，ncnn框架是用专用的模型格式，故需要从onnx格式转为ncnn专用格式，理论上是一样的模型，但转换可能造成精度损失。
5. ncnn官方提供了android平台的预编译库，onnxruntime则需要自己编译。
6. 对比测试的项目有：推理速度、编译后apk文件大小

#### 推理速度对比
1. 在同一手机以及同一个PC机(模拟器)上进行测试，对比arm64-v8a以及x86_64这2种ABI。
2. onnxruntime版本为最新HEAD20201020，ncnn版本tag 20200916
3. 测试条件：不计算模型加载、框架初始化、加载图片的时间，以同一张图片，同样的参数，跑100次计算平均耗时。
4. 测试手机，华为荣耀play1，麒麟970，4*Cortex-A73+4*Cortex-A53
5. 测试PC，Intel i5-9400F，6核心6线程，macOS 10.15.7，google模拟器 android 9.0 x86_64，模拟器最多只能配置3个核心
6. 模拟器是3个核心，真机是4个大核+4个小核，20201130更新：angleNet阶段和crnnNet阶段启用OpenMP后，ncnn版提示效果明显，但模拟器核心数量太少，模拟器的测试结果仅供参考。

| ABI    | 线程数  | onnx耗时 | ncnn耗时 | 耗时倍率 |
| ------- | ------- | ----: | ----: | ----: |
| arm64-v8a | 1  | 1454.54ms | 1564.57ms | 0.93 |
| x86_64    | 1  | 481.74ms | 354.60ms | 1.35 |
| arm64-v8a | 4  | 919.93ms | 749.23ms | 1.23 |
| x86_64    | 4  | 660.72ms | 243.83ms | 2.71|
| arm64-v8a | 6  | 1200.28ms | 756.18ms | 1.59 |
| x86_64    | 6  | 1296.30ms | 268.10ms | 4.83 |

#### 推理速度对比总结：
1. 1个线程的情况下，arm平台两者速度相当，x86平台ncnn更快。
2. 多线程的情况下，ncnn更快。
3. 总体来说，android端ncnn推理速度更快。

#### 文件大小对比
1. onnxruntime为自己编译的动态库，ncnn为静态库。
2. onnxruntime的大小还能通过自己编译mini_build选项进一步优化，但因为模型格式也得转为ort专用格式，故这里就没有加入对比。
3. 直接对比编译后apk大小。

| 平台     | onnx大小 | ncnn大小 | Size倍率 |
| ------- | -------- | ------: | ------: |
| android |  69.66M  |  60.35M |   1.15倍 |


#### 文件大小对比总结：
1. ncnn在android平台文件大小项目完胜onnx。
2. 没有对比onnx静态编译的情况，所以此对比不够严谨。

#### 总体总结：
1. ncnn为移动端优化，模型大小和文件大小都更小，适合移动端部署。
2. ncnn在移动端的推理速度更快。